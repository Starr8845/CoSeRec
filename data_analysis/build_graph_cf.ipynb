{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "296337\n",
      "(35599, 18358)\n",
      "(18358, 18358)\n",
      "Graph(num_nodes=18358, num_edges=385482,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})\n",
      "{'w': tensor([1.0000, 0.1429, 0.1543,  ..., 0.5855, 0.4558, 1.0000])}\n",
      "tensor(0.1713)\n",
      "16.142997221768262\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def build_sim_graph(cf_graph, k, user_num, item_num):\n",
    "    import dgl\n",
    "    \n",
    "    similarity = cosine_similarity(cf_graph.transpose())\n",
    "    # filter topk connections\n",
    "    sim_items_slices = []\n",
    "    sim_weights_slices = []\n",
    "    i = 0\n",
    "    print(similarity.shape) # [3417, 3417]\n",
    "    while i < similarity.shape[0]:     \n",
    "        end = min(similarity.shape[0], i+256)\n",
    "        # print(i, end)\n",
    "        sim = similarity[i:end, :] # 改一下\n",
    "        sim_items = np.argpartition(sim, -(k+1), axis=1)[:, -(k+1):]\n",
    "        sim_weights = np.take_along_axis(sim, sim_items, axis=1)\n",
    "        sim_items_slices.append(sim_items)\n",
    "        sim_weights_slices.append(sim_weights)\n",
    "        i = i + 256\n",
    "\n",
    "    sim_items = np.concatenate(sim_items_slices, axis=0)\n",
    "    sim_weights = np.concatenate(sim_weights_slices, axis=0)\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(len(sim_items)):\n",
    "        row.extend([i]*len(sim_items[i]))\n",
    "        col.extend(sim_items[i])\n",
    "    # values = sim_weights / sim_weights.sum(axis=1, keepdims=True) # 这一行先暂时注释掉\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    \n",
    "    values = sim_weights\n",
    "    \n",
    "    values = np.nan_to_num(values).flatten()\n",
    "    # 把权重为0的边删掉\n",
    "    row = row[values>0]\n",
    "    col = col[values>0]\n",
    "    values = values[values>0]\n",
    "    adj_mat = csr_matrix((values, (row, col)), shape=(\n",
    "        item_num + 1, item_num + 1))\n",
    "    g = dgl.from_scipy(adj_mat, 'w')\n",
    "    g.edata['w'] = g.edata['w'].float()\n",
    "    return g\n",
    "\n",
    "def generate_rating_matrix_valid(user_seq, num_users, num_items):\n",
    "    # three lists are used to construct sparse matrix\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for user_id, item_list in user_seq.items():\n",
    "        for item in item_list:\n",
    "            row.append(user_id)\n",
    "            col.append(item)\n",
    "            data.append(1)\n",
    "\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    data = np.array(data)\n",
    "    rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items), dtype=np.float32)\n",
    "\n",
    "    return rating_matrix\n",
    "\n",
    "# 数据读进来\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# dataset_name = \"ml-1m\"\n",
    "# dataset_name = \"Yelp\"\n",
    "# dataset_name = \"Toys_and_Games\"\n",
    "# dataset_name = \"Sports_and_Outdoors\"\n",
    "dataset_name = \"Beauty\"\n",
    "\n",
    "\n",
    "with open(f\"/home/zzx/seqRec/CLTrys/CoSeRec/data/{dataset_name}.txt\",\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "user_seq = {}\n",
    "for each in raw_data:\n",
    "    each = [int(temp) for temp in each.strip().split(' ')]\n",
    "    user_seq[each[0]] = each[1:]\n",
    "\n",
    "\n",
    "item_frequency = defaultdict(int)\n",
    "\n",
    "for seq in user_seq.values():\n",
    "    for each_item in seq:\n",
    "        item_frequency[each_item] +=1\n",
    "\n",
    "\n",
    "rating_matrix = generate_rating_matrix_valid(user_seq, len(user_seq)+1, len(item_frequency)+1)\n",
    "# 这里rating matrix有问题，需要仔细看一下\n",
    "# print(rating_matrix)\n",
    "print(type(rating_matrix))\n",
    "\n",
    "print(rating_matrix.count_nonzero())\n",
    "print(rating_matrix.shape)\n",
    "\n",
    "g = build_sim_graph(rating_matrix, 20, len(user_seq), len(item_frequency))\n",
    "\n",
    "\n",
    "import torch \n",
    "print(g)\n",
    "# 算一下边的权重   cosine similarity\n",
    "print(g.edata)\n",
    "print(torch.mean(g.edata['w']))\n",
    "temp_weights = g.edata['w']\n",
    "# \n",
    "\n",
    "\n",
    "# 下面要看一下 low frequency 的 item的边的分布\n",
    "\n",
    "item_fre_threshhold = np.average(list(item_frequency.values()))\n",
    "print(item_fre_threshhold)\n",
    "tail_nodes = np.array([node for node, freq in item_frequency.items() if freq<item_fre_threshhold])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长尾节点平均度,  45.87143474503025\n"
     ]
    }
   ],
   "source": [
    "a = (g.in_degrees(tail_nodes)+g.out_degrees(tail_nodes)).numpy()\n",
    "print(\"长尾节点平均度, \", np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有节点的度, 41.99607800413988\n"
     ]
    }
   ],
   "source": [
    "b = (g.in_degrees()+g.out_degrees()).numpy()\n",
    "print(\"所有节点的度,\", np.mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18358])\n",
      "torch.Size([18358])\n",
      "tensor([0.0000, 0.1593, 0.1537,  ..., 0.3529, 0.3813, 0.3214])\n",
      "tensor(0.1928)\n",
      "0.1744557\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "g.update_all(dgl.function.copy_e('w', 'm'), dgl.function.sum('m', 'sum_w'))\n",
    "\n",
    "in_degrees = g.in_degrees().float()\n",
    "    \n",
    "print(g.ndata['sum_w'].shape)\n",
    "print(in_degrees.shape)\n",
    "# 计算平均权重\n",
    "avg_weights = g.ndata['sum_w'] / in_degrees\n",
    "\n",
    "# 处理 in_degrees 为 0 的情况\n",
    "avg_weights[in_degrees == 0] = 0\n",
    "\n",
    "print(avg_weights)\n",
    "print(torch.mean(avg_weights))\n",
    "\n",
    "print(np.mean(avg_weights.numpy()[tail_nodes]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
